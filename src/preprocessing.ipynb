{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3077a741-db52-4d5f-a94a-f8f3b144dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import torch; torch.utils.backcompat.broadcast_warning.enabled = True\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.backends.cudnn as cudnn; cudnn.benchmark = True\n",
    "from scipy.fftpack import fft, rfft, fftfreq, irfft, ifft, rfftfreq\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import importlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c280e978-1a38-4ba9-80cc-d5ebdcab7096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "# print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30859550-c766-4fdc-b52a-41e38569f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_signals_path = \"/media/NAS/EEG2IMAGE/eeg_cvpr_2017/data/eeg_5_95_std.pth\"\n",
    "img_path = '/media/NAS/EEG2IMAGE/eeg_cvpr_2017/image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b75008-dd13-4181-a159-3cb8c16fd2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Load...\n"
     ]
    }
   ],
   "source": [
    "class EEGDataset:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, eeg_signals_path, eeg_data_path):\n",
    "        # Load EEG signals\n",
    "        print(\"Start Load...\")\n",
    "        loaded = torch.load(eeg_signals_path)\n",
    "        # if opt.subject!=0:\n",
    "        #     self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==0]\n",
    "        # else:\n",
    "        self.data=loaded['dataset']        \n",
    "        self.labels = loaded[\"labels\"]\n",
    "        self.images = loaded[\"images\"]\n",
    "        self.image_path = eeg_data_path\n",
    "        \n",
    "        # Compute size\n",
    "        self.size = len(self.data)\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        # Process EEG\n",
    "        # print(self.data[i].keys())\n",
    "        eeg = self.data[i][\"eeg\"].float().t()\n",
    "        eeg = eeg[20:460,:]\n",
    "\n",
    "        # if opt.model_type == \"model10\":\n",
    "        #     eeg = eeg.t()\n",
    "        #     eeg = eeg.view(1,128,460-20)\n",
    "        # Get label        \n",
    "        label = self.data[i][\"label\"]\n",
    "\n",
    "        # Get Original Image\n",
    "        image = self.images[self.data[i][\"image\"]]\n",
    "\n",
    "        subject = self.data[i]['subject']\n",
    "\n",
    "        # Return\n",
    "        return eeg, image, label, subject\n",
    "\n",
    "# Splitter class\n",
    "class Splitter:\n",
    "\n",
    "    def __init__(self, dataset, split_path, split_num=0, split_name=\"train\"):\n",
    "        # Set EEG dataset\n",
    "        self.dataset = dataset\n",
    "        # Load split\n",
    "        loaded = torch.load(split_path)\n",
    "        self.split_idx = loaded[\"splits\"][split_num][split_name]\n",
    "        # Filter data\n",
    "        self.split_idx = [i for i in self.split_idx if 450 <= self.dataset.data[i][\"eeg\"].size(1) <= 600]\n",
    "        # Compute size\n",
    "        self.size = len(self.split_idx)\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        # Get sample from dataset\n",
    "        eeg, image, label, subject = self.dataset[self.split_idx[i]]\n",
    "        # Return\n",
    "        return eeg, image, label, subject\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = EEGDataset(eeg_signals_path = eeg_signals_path,  eeg_data_path = img_path)\n",
    "# Create loaders\n",
    "loaders = {split: DataLoader(Splitter(dataset, split_path = \"/media/NAS/EEG2IMAGE/eeg_cvpr_2017/data/block_splits_by_image_all.pth\", \n",
    "                                      split_num = 0, \n",
    "                                      split_name = split), 1, drop_last = False, shuffle = False) for split in [\"train\", \"val\", \"test\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b97e5828-dfbc-453a-bfa9-800f995fe110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Load...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class EEGPreDataset:\n",
    "\n",
    "    # Constructo\n",
    "    def __init__(self, eeg_pre_path, eeg_data_path, transforms=None):\n",
    "        # Load EEG signals\n",
    "        print(\"Start Load...\")\n",
    "        # loaded = torch.load(eeg_signals_path)\n",
    "\n",
    "        # split_loaded = torch.load(split_path)\n",
    "        # if opt.subject!=0:\n",
    "        #     self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==0]\n",
    "        # # else:\n",
    "        # self.data=loaded['dataset']        \n",
    "        # self.labels = loaded[\"labels\"]\n",
    "        # self.images = loaded[\"images\"]\n",
    "        self.image_path = eeg_data_path\n",
    "        self.data = glob.glob(os.path.join(eeg_pre_path, \"*\"))\n",
    "\n",
    "        # Compute size\n",
    "        self.dataset_size = len(self.data)\n",
    "\n",
    "        \n",
    "        # Transforms\n",
    "        self.transforms = transforms\n",
    "        # self.to_tensor  = ToTensor()\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        loaded = torch.load(self.data[i])\n",
    "        # Process EEG\n",
    "        eeg = loaded[\"eeg\"]\n",
    "\n",
    "        # Get label        \n",
    "        label = loaded[\"label\"]\n",
    "\n",
    "        # Get Original Image\n",
    "        image_name = loaded[\"image\"]\n",
    "        s, _ = image_name.split(\"_\")\n",
    "        image = torch.empty((224,224))\n",
    "        if os.path.exists(os.path.join(self.image_path, s, image_name+\".JPEG\")):\n",
    "            image = cv2.imread(os.path.join(self.image_path, s, image_name+\".JPEG\"))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.\n",
    "        else:\n",
    "            print(os.path.join(self.image_path, s, image_name+\".JPEG\"))\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        # \n",
    "        # image = self.to_tensor(image)\n",
    "        \n",
    "        # Return\n",
    "        return eeg, image, label\n",
    "\n",
    "dataset = EEGPreDataset(os.path.join(\".\",\"preprocessing_data\",\"train\") , img_path)\n",
    "loaders = DataLoader(dataset, 1, drop_last = False, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59e08926-8820-4c9e-a105-3eb91766b160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eeg', 'image', 'label', 'subject'])\n",
      "[tensor([[[-0.2232, -0.0730, -0.1207,  ..., -0.4589, -0.0330, -0.2386],\n",
      "         [-0.2237, -0.0651, -0.1656,  ..., -0.4115, -0.0304, -0.2057],\n",
      "         [-0.1934, -0.0134, -0.2735,  ..., -0.3696, -0.0285, -0.1856],\n",
      "         ...,\n",
      "         [ 0.3540,  0.2597,  0.3852,  ...,  0.3877,  0.0131,  0.1309],\n",
      "         [ 0.3000,  0.1950,  0.4775,  ...,  0.3140,  0.0086,  0.0727],\n",
      "         [ 0.2586,  0.1368,  0.5080,  ...,  0.2619,  0.0052,  0.0441]]]), ('n02951358_31190',), tensor([10]), tensor([4])]\n"
     ]
    }
   ],
   "source": [
    "for l in loaders[\"train\"]:\n",
    "    print(l)\n",
    "    test = l\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "973d1faf-af23-4b00-9084-1e798c082eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22159b3f-556b-4265-be01-88fa80fc45bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3baa5944c1d41e68bc1eaea69fcc42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train data preprocessing...:   0%|          | 0/7959 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22318df16d9141ea990a17db5d5477ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val data preprocessing...:   0%|          | 0/1994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0ba0937a6247a88acfd9518b11513e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test data preprocessing...:   0%|          | 0/1987 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os,sys\n",
    "from tqdm.notebook  import tqdm\n",
    "\n",
    "path = os.path.join(\"/media/NAS/EEG2IMAGE/eeg_cvpr_2017\",\"preprocessing_data\")\n",
    "file_name  = eeg_signals_path.split(\"/\")[-1].replace(\".pth\", \"\")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    for idx, data in tqdm(enumerate(loaders[split]), total = len(loaders[split]), desc = f\"{split} data preprocessing...\"):\n",
    "        subject = data[2].item()\n",
    "        data = {\"eeg\":data[0].numpy().squeeze(), \"image\":data[1][0], \"label\":data[2].item(), \"subject\":data[3].item()}\n",
    "        # torch.save(data, os.path.join(path, split, f\"{file_name}_{idx}.pth\"))\n",
    "        if not os.path.exists(os.path.join(path, f\"class_{subject}\")): os.mkdir(os.path.join(path, f\"class_{subject}\"))\n",
    "        torch.save(data, os.path.join(path, f\"class_{subject}\", f\"{file_name}_{idx}.pth\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68da383a-94d6-480d-8847-9c071c319c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e81551509d140c78429cb1cd26873eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessing...:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os,sys\n",
    "from tqdm.notebook  import tqdm\n",
    "from random import shuffle\n",
    "import shutil\n",
    "\n",
    "frac = 0.8\n",
    "path = glob.glob(os.path.join(\"/media/NAS/EEG2IMAGE/eeg_cvpr_2017\",\"preprocessing_data\", \"by_class\", \"*\"))\n",
    "train_path = os.path.join(\"/media/NAS/EEG2IMAGE/eeg_cvpr_2017\",\"preprocessing_data\",\"train\")\n",
    "test_path = os.path.join(\"/media/NAS/EEG2IMAGE/eeg_cvpr_2017\",\"preprocessing_data\",\"test\")\n",
    "valid_path = os.path.join(\"/media/NAS/EEG2IMAGE/eeg_cvpr_2017\",\"preprocessing_data\",\"val\")\n",
    "tr_idx = 0\n",
    "te_idx = 0\n",
    "va_idx = 0\n",
    "\n",
    "for i, p in tqdm(enumerate(path), desc=f\"preprocessing...\", total=len(path)):\n",
    "    ep_lst = os.listdir(p)\n",
    "    name   = p.split(\"/\")[-1]\n",
    "    shuffle(ep_lst)\n",
    "    length = len(ep_lst)\n",
    "    train_len  = int(length * frac)\n",
    "    valid_len  = int(length * (1-frac)//2)\n",
    "    for idx, t in enumerate(ep_lst[:train_len]):\n",
    "        shutil.copy2(os.path.join(p, t), os.path.join(train_path, f\"{name}_{tr_idx}.pth\"))\n",
    "        tr_idx +=1\n",
    "\n",
    "    for idx, t in enumerate(ep_lst[train_len:train_len+valid_len]): \n",
    "        shutil.copy2(os.path.join(p, t), os.path.join(valid_path, f\"{name}_{va_idx}.pth\"))\n",
    "        va_idx +=1\n",
    "\n",
    "    for idx, t in enumerate(ep_lst[train_len+valid_len:]): \n",
    "        shutil.copy2(os.path.join(p, t), os.path.join(test_path, f\"{name}_{te_idx}.pth\"))\n",
    "        te_idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e4929b2-4880-4007-b1db-7e9d2ede09a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class_31'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f65dcd09-c784-4dd5-8ddb-2b8906b15647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len + valid_len + valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f29be389-a73e-4953-8737-1790d4f988fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(valid_len)\n",
    "train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b60aa428-7b40-4dc2-b1c4-5430cfbf43e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class_31',\n",
       " 'class_5',\n",
       " 'class_21',\n",
       " 'class_20',\n",
       " 'class_6',\n",
       " 'class_2',\n",
       " 'class_30',\n",
       " 'class_37',\n",
       " 'class_22',\n",
       " 'class_23',\n",
       " 'class_11',\n",
       " 'class_18',\n",
       " 'class_15',\n",
       " 'class_26',\n",
       " 'class_29',\n",
       " 'class_39',\n",
       " 'class_7',\n",
       " 'class_24',\n",
       " 'class_12',\n",
       " 'class_9',\n",
       " 'class_8',\n",
       " 'class_36',\n",
       " 'class_13',\n",
       " 'class_25',\n",
       " 'class_17',\n",
       " 'class_4',\n",
       " 'class_34',\n",
       " 'class_16',\n",
       " 'class_3',\n",
       " 'class_33',\n",
       " 'class_28',\n",
       " 'class_27',\n",
       " 'class_0',\n",
       " 'class_10',\n",
       " 'class_35',\n",
       " 'class_19',\n",
       " 'class_38',\n",
       " 'class_14',\n",
       " 'class_1',\n",
       " 'class_32']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e47e4a-62fa-4cb7-bbd6-c7f9564901ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['eeg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953599d6-7322-4a20-8533-e1770f5d8da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.load(os.path.join(path, split, f\"{file_name}_{idx}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c2739-96b2-4e32-9a7d-24438e503b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = dataset.images[dataset.data[1][\"image\"]]\n",
    "s, _ = image_name.split(\"_\")\n",
    "image = cv2.imread(os.path.join(dataset.image_path, s, image_name+\".JPEG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6f6225-3b33-4aae-b911-f4350f518c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for l in loaders:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
